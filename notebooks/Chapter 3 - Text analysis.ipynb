{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Text Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- *A Python Course for the Humanities by Folgert Karsdorp and Maarten van Gompel*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will introduce you to the task of text analysis in Python. You will learn how to read an entire corpus into Python, clean it and how to perform certain data analyses on those texts. We will also briefly introduce you to using Python's plotting library *matplotlib*, with which you can visualize your data.\n",
    "\n",
    "Before we delve into the main subject of this chapter, text analysis, we will first write a couple of utility functions that build upon the things you learnt in the previous chapter. Often we don't work with a single text file stored at our computer, but with multiple text files or entire corpora. We would like to have a way to load a corpus into Python.\n",
    "\n",
    "Remember how to read files? Each time we had to open a file, read the contents and ensure that the file is closed. Since this is a series of steps we will often need to do, we can write a single function that does all that for us. We write a small utility function `read_file(filename)` that reads the specified file and simply returns all contents as a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    \"\"\"Read the contents of `filename` and return as a string.\"\"\"\n",
    "    with open(filename) as infile:\n",
    "        contents = infile.read()\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of having to remember the steps to read a file, we can just call the function `read_file`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_file('data/austen-emma-excerpt.txt')\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the directory `data/gutenberg/training` we have a corpus consisting of multiple files with the extension `.txt`. This corpus is a collection of English novels which we downloaded for you from the [Gutenberg](http://www.gutenberg.org) project. We want to iterate over all the filenames. You can do this using the `glob` function from the `glob` module in the Python standard library. We can import this function as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, the `glob` function is available to use. This function takes as argument a pattern for filenames and returns all the matching files and directories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glob('data/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `glob` returns a list and we can iterate over that list. A `*` in a pattern stands for any directory or filename; the result above lists all the files and directories in the `data` directory. We can make the pattern more specific and select only files ending with `.txt`. The following code will read all text files in the directory `data/gutenberg/training` and outputs the length (in characters) of each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filepath in glob('data/gutenberg/training/*.txt'):\n",
    "    text = read_file(filepath)\n",
    "    print(f'{filepath} has {len(text)} characters.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Text Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> When the next night came, Dinarazad said to her sister Shahrazad: ‘In God’s name, sister, if you are not asleep, then tell us one of your stories!’ Shahrazad answered: ‘With great pleasure! I have heard tell, honoured King, that…’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Alf Laylah Wa Laylah*, *the Stories of One Thousand and One Nights* is a collection of folk tales, collected over many centuries by various authors, translators, and scholars across West, Central and South Asia and North Africa, forms a huge narrative wheel with an overarching plot, created by the frame story of Shahrazad.\n",
    "\n",
    "The stories begin with the tale of king Shahryar and his brother, who, having both been deceived by their respective Sultanas, leave their kingdom, only to return when they have found someone who — in their view — was wronged even more. On their journey the two brothers encounter a huge jinn who carries a glass box containing a beautiful young woman. The two brothers hide as quickly as they can in a tree. The jinn lays his head on the girl’s lap and as soon as he is asleep, the girl demands the two kings to make love to her or else she will wake her ‘husband’. They reluctantly give in and the brothers soon discover that the girl has already betrayed the jinn ninety-eight times before. This exemplar of lust and treachery strengthens the Sultan’s opinion that all women are wicked and not to be trusted. \n",
    "\n",
    "When king Shahryar returns home, his wrath against women has grown to an unprecedented level. To temper his anger, each night the king sleeps with a virgin only to execute her the next morning. In order to make an end to this cruelty and save womanhood from a \"virgin scarcity\", Sharazad offers herself as the next king’s bride. On the first night, Sharazad begins to tell the king a story, but she does not end it. The king’s curiosity to know how the story ends, prevents him from executing Shahrazad. The next night Shahrazad finishes her story, and begins a new one. The king, eager to know the ending of this tale as well, postpones her execution once more. Using this strategy for One Thousand and One Nights in a labyrinth of stories-within-stories-within-stories, Shahrazad attempts to gradually move the king’s cynical stance against women towards a politics of love and justice (see Marina Warner’s *Stranger Magic* (2013) in case you're interested).\n",
    "\n",
    "The first European version of the Nights was translated into French by Antoine Galland. Many translations (in different languages) followed, such as the (heavily criticized) English translation by Sir Richard Francis Burton entitled *The Book of the Thousand and a Night* (1885). This version is freely available from the Gutenberg project (see [here](http://www.gutenberg.org)), and will be the one we will explore here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the directory `data/arabian_nights` you will find 999 files. This is because in Burton's translation some nights are missing. The name of the file represents the corresponding night of storytelling in *Alf Laylah Wa Laylah*. Go have a look. Use `glob` to get a list of all the file names. Store the result in a variable named `filenames`.\n",
    "\n",
    "Use the tokenize function and the corpus reading function we have defined above and tokenize and clean each night. Store the result in the variable named `corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job! You now should have a list of 999 file names. It is always important to check whether our code actually produces the desired results. Let's check whether we indeed have 999 texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, that seems to be correct. It would be convenient for further processing to have the corpus in chronological order. Let's have a look the first 20 file names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the files are sorted by their string name and not by their numbering. To be able to sort the files by their numbers we must first remove the extension `.txt` as well as the directory `data/arabian_nights/`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Write a function `remove_txt` that takes as argument a string and some extension that you want to remove. It should return the string without the extension. Since filenames are just strings, we could strip off the extension by looking for the period. However, since manipulating filenames is so common, there are functions for this in the standard library. This makes it both easier to manipulate filenames correctly, and makes it more obvious what you are doing to others reading your code. See the function `splitext` from the `os.path` module. Look up the documentation [here](http://docs.python.org/3/library/os.path.html#os.path.splitext)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import splitext\n",
    "\n",
    "def remove_ext(filename):\n",
    "    # insert your code here\n",
    "    \n",
    "# these tests should return True if your code is correct\n",
    "print(remove_ext(\"data/arabian_nights/1.txt\") == \"data/arabian_nights/1\")\n",
    "print(remove_ext(\"ridiculous_selfie.jpg\") == \"ridiculous_selfie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Write a function `remove_dir` that takes as argument a filepath and removes the directory from a filepath. Tip: use the function `basename` from the `os.path` module. Look up the documentation [here](http://docs.python.org/3/library/os.path.html#os.path.basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename\n",
    "\n",
    "def remove_dir(filepath):\n",
    "    # insert your code here\n",
    "    \n",
    "# these tests should return True if your code is correct\n",
    "print(remove_dir(\"data/arabian_nights/1.txt\") == \"1.txt\")\n",
    "print(remove_dir(\"/a/kind/of/funny/filepath/to/file.txt\") == \"file.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** Combine the two functions `remove_ext` and `remove_dir` into one function `remove_dir_ext`. This function takes as argument a filepath and returns the name (without the extensions) of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_dir_ext(filepath):\n",
    "    # insert your code here\n",
    "    \n",
    "# these tests should return True if your code is correct\n",
    "print(remove_dir_ext(\"data/arabian_nights/1.txt\") == '1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to convert numbers represented as string (e.g. \"1\" and \"10\") to a number. This can be achieved by using the function `int`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_as_string = \"1\"\n",
    "x_as_int = int(x_as_string)\n",
    "print(x_as_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that strings are different types than integers. To see this, have a look at the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"1\"\n",
    "y = \"2\"\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12? Yes, 12. This is because, as you might remember from the first chapter, we can use the `+` operator to concatenate two strings. If we apply the same operation to integers, as in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 1\n",
    "y = 2\n",
    "print(x + y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get the expected result of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the functions `int` and `remove_dir_ext` into the function `get_night` to obtain the integer corresponding to a night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_night(filepath):\n",
    "    # insert your code here\n",
    "\n",
    "# these tests should return True if your code is correct\n",
    "print(get_night(\"data/arabian_nights/1.txt\") == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so now we can convert the filepaths to integers corresponding to the nights of storytelling. But how will we use that to sort the corpus in chronological order? In chapter 1 we briefly discussed how to sort your collection of good reads. In combination with our `get_night` function, we can use `sort` to obtain a nicely chronologically ordered list of stories. Prepare yourself for some real Python magic, because the following lines of code might be a little dazzling...\n",
    "\n",
    "We start with the variable `filenames`, which already has the list of file names we want to sort.\n",
    "Next we call the function `sorted()` on this list and supply as keyword our function `get_night`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = sorted(filenames, key=get_night)\n",
    "filenames[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A keyword argument is an argument that you specify by name. Keyword arguments must come after normal arguments.\n",
    "\n",
    "As you can see, we now have a perfectly chronologically ordered list of filenames. But how, **HOW!** did that work? As you might have guessed, the keyword argument `key=get_night` of `sorted` has something to do with all this magic. Without this argument, Python would just sort the filenames alphabetically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob('data/arabian_nights/*.txt')\n",
    "filenames = sorted(filenames)\n",
    "print(filenames[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we supply a function to `key`, Python will apply that function to each item before comparing it to other items during the sorting. In our case this means Python converts all filepaths to integers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence splitting and Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous chapter we wrote a simple function to clean a text so that we could split it into a list of words (\"tokenize\"). For example:\n",
    "\n",
    "```python\n",
    "cleaned = clean_text('This is a sentence. Should we seperate it from this one?')\n",
    "tokens = cleaned.split()\n",
    "print(tokens)\n",
    "```\n",
    "gave:\n",
    "```\n",
    "['this', 'is', 'a', 'sentence', 'should', 'we', 'seperate', 'it', 'from', 'this', 'one']\n",
    "```\n",
    "To recap, this is a list of strings. Can you extract the first token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['this', 'is', 'a', 'sentence', 'should', 'we', 'seperate', 'it', 'from', 'this', 'one']\n",
    "# replace ... with your code\n",
    "print(... == 'this')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the first character of the second token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(... == 'i')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using this function we lose information about where sentences end and start in the text. What if we want to know about both tokens and sentence boundaries? The sentence is an important unit of analysis in language. For example, longer sentences are typically considered more difficult.\n",
    "\n",
    "We can preserve information on sentences by adding another level of lists. We can represent a text as a list of sentences, where each sentence is a list of strings (tokens).\n",
    "\n",
    "Rather than taking our simple tokenizer and adding a rudimentary sentence splitter to it, we will now turn to a library that does tokenization and sentence splitting, and handles some of the harder edge cases. A simple strategy would split sentences whenever sentence-ending punctuation is encountered (`?!.`), but consider for example:\n",
    "\n",
    "- The use of `.` as end-of-sentence marker vs. in abbreviations and initials\n",
    "- Question marks inside direct speech: `\"How are you?\", she asked.`\n",
    "\n",
    "Similarly, consider difficult tokenization cases:\n",
    "\n",
    "- John's bike\n",
    "- New York-based\n",
    "- nineteenth- and twentieth-century writers\n",
    "- (SAM)-missile launchers\n",
    "- i.e.  Ph.D. &c.\n",
    "- ;-) :))) #yolo#lit#hashtag\n",
    "- https://en.wikipedia.org/wiki/Bicycle\n",
    "- 8-Cyclopentyl-1,3-dimethylxanthine\n",
    "\n",
    "The [Spacy](https://spacy.io/) library includes a good tokenizer and sentence splitter, among other Natural Language Processing (NLP) tasks. Install Spacy using the Anaconda Navigator (follow [these instructions](https://docs.anaconda.com/anaconda/navigator/getting-started/#navigator-managing-packages)).\n",
    "\n",
    "Spacy works rather different than the code we wrote until now. A proper introduction of Spacy is beyond the scope of this notebook, but the documentation of Spacy is excellent and they provide an [online course](https://course.spacy.io/) which you can follow.\n",
    "\n",
    "For now, we will use the following code to do lower casing, punctuation elimination, tokenization, and sentence splitting with Spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "sentencizer = nlp.create_pipe(\"sentencizer\") # Add sentence splitting\n",
    "nlp.add_pipe(sentencizer)\n",
    "\n",
    "def tokenize_sentence(sent):\n",
    "    \"\"\"Given a sentence, return lowercased tokens without punctuation and whitespace.\"\"\"\n",
    "    return [token.lower_ for token in sent if not token.is_punct and not token.is_space]\n",
    "\n",
    "def preprocess(text):\n",
    "    \"\"\"Given a document as string, return it as a list of sentences,\n",
    "    where each sentence is a list of cleaned words.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    return [tokenize_sentence(sent) for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1: We import the library and specify the language we will use (Spacy supports more languages).\n",
    "- 2: We create an object to do NLP tasks for English text\n",
    "- 3-4: By default, sentences are not split, so we add it to the pipeline. Spacy support several levels of analysis; for now we are only concerned with tokenization and sentence splitting.\n",
    "- 6-8: The `tokenize_sentence` function handles a single sentence: the lower-case version of tokens is used, and punctuation and whitespace is filtered. Since tokens are special objects for Spacy, they provide this information as pre-computed attributes (i.e., we don't have to call a method to get this information).\n",
    "- 10-14: The `preprocess` function takes a text as string and applies the NLP pipeline to it. It returns the result as a list of sentences, which in turn are lists of tokens.\n",
    "\n",
    "Note: we could also return the `doc` result directly, but to use this object, we have to know how Spacy works. By converting everything to lists and strings we can work with datastructures that we already know in the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the code:\n",
    "preprocess('This is a sentence. Should we seperate it from this one?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# What about tricky cases? (This is a single string spread over several lines)\n",
    "text = \"\"\"She asked: \"Is Dr. Phil a doctor?\". \"Dr. Phil has a\n",
    "Ph.D. but is not an M.D.\" I replied. \"He even lost\n",
    "his license to practice therapy,\" I added.\"\"\"\n",
    "preprocess(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for readability, Python prints long lists with one element per line. Remember that within lists, whitespace is completely free and not significant.\n",
    "\n",
    "We can now go back to our sorted list of filenames and read our corpus. We read each file, apply the preprocessing function to it, and collect the result in one big `corpus` list (this may take a minute!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "filenames = glob('data/arabian_nights/*.txt')\n",
    "filenames = sorted(filenames, key=get_night)\n",
    "for filename in filenames:\n",
    "    text = read_file(filename)\n",
    "    corpus.append(preprocess(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `preprocess` function already returned two levels of lists (and strings), and now we added yet another level. Do you understand the meaning of each level? Try the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus[0][0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decribe each level in one word:\n",
    "\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "4. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first exploratory data analysis, we are going to compute for each night how many sentences it contains and how many words. It is quite easy to count the number of sentences per night, since each night is represented by a list of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_per_night = []\n",
    "for night in corpus:\n",
    "    sentences_per_night.append(len(night))\n",
    "print(sentences_per_night[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the function `max` we can find out what the highest number of sentences is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(sentences_per_night)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, if we would like to now what the lowest number of sentences is, we use the function `min`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(sentences_per_night)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `sum` takes a list of numbers as input and returns the sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum([1, 3, 3, 4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to compute [the average](https://en.wikipedia.org/wiki/Average#Arithmetic_mean) number of sentences per night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our data structure of a list of sentences which are themselves lists of words, it is a little trickier to count for each night how many words it contains. One possible way is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_night = []\n",
    "for night in corpus:\n",
    "    n_words = 0\n",
    "    for sentence in night:\n",
    "        n_words += len(sentence)\n",
    "    words_per_night.append(n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you really understand these lines of code as you will need them in the next quiz. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The suspense created by Shahrazad’s story-telling skills is intriguing, especially the “cliff-hanger” ending each night which she uses to avert her own execution (and possibly that of womanhood). Every night she tells the Sultan a story only to stop at dawn and she picks up the thread the next night. But does it really take the whole night to tell a particular story?\n",
    "\n",
    "I am not aware of any exact numbers about how many words people speak per minute. Averages seem to fluctuate between 100 and 200 words per minute. Narrators are advised to use approximately 150 words per minute in audiobooks. I suspect that this number is a little lower for live storytelling and assume it lies around 130 words per minute (including pauses). Using this information, we can compute the time it takes to tell a particular story as follows:\n",
    "\n",
    "$$\\textrm{story time}(\\textrm{text}) = \\frac{\\textrm{number of words in text}}{\\textrm{number of words per minute}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Write a function called `story_time` that takes as input a text. Given a speed of 130 words per minute, compute how long it takes to tell that text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def story_time(text):\n",
    "    # insert your code here\n",
    "\n",
    "# these tests should return True if your code is correct\n",
    "print(story_time([[\"story\", \"story\"]]) * 130 == 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** Compute the story_time for each night in our corpus. Assign the result to the variable `story_time_per_night`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_time_per_night = []\n",
    "# insert your code here\n",
    "print(story_time_per_night[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**) Compute the average, minimum and maximum story telling time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing general statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed a range of general statistics for our corpus, it would be nice to visualize them. Python's plotting library *matplotlib* (see [here](http://matplotlib.org)) allows us to produce all kinds of graphs. In addition to importing the library, we need to issue a special command to ensure that our plots will be displayed in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can, for example, plot for each story how many sentences it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sentences_per_night);  # ';' at the end suppresses the return value, we only want to see the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1)** Can you do the same for `words_per_night`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2)** And can you do the same for `story_time_per_night`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3)** In this final exercise we will put everything together what we have learnt so far. We want you to write a function `positions_of` that returns for a given word all sentence positions in the *Arabian Nights* where that word occurs. We are not interested in the positions relative to a particular night, but only to the corpus as a whole. Use that function to find all occurrences of the name *Sharahzad* and store the corresponding indices in the variable `positions_of_shahrazad`. Do the same thing for the name *Ali*. Store the result in `positions_of_ali`. Finally, find all occurrences of *Egypt* and store the indices in `positions_of_egypt`. Hint: to compute the position of a word in the whole corpus, you will need keep track of a running total of words as you through each night and sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positions_of(word):\n",
    "    # insert your code here\n",
    "\n",
    "positions_of_shahrazad = positions_of('shahrazad')\n",
    "positions_of_ali = positions_of('ali')\n",
    "positions_of_egypt = positions_of('egypt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything went well, the following lines of code should produce a nice [dispersion plot](https://medium.com/the-political-ear/tutorial-plotting-lexical-dispersion-conspiracy-lies-from-the-left-of-center-c0b39de442d5) of all sentence occurrences of Shahrazad, Ali and Egypt in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "names = ['Shahrazad', 'Ali', 'Egypt']\n",
    "plt.plot(positions_of_shahrazad, [1] * len(positions_of_shahrazad), '|', markersize=100)\n",
    "plt.plot(positions_of_ali, [2] * len(positions_of_ali), '|', markersize=100)\n",
    "plt.plot(positions_of_egypt, [0] * len(positions_of_egypt), '|', markersize=100)\n",
    "plt.yticks(range(len(names)), names)\n",
    "plt.ylim(-1, 3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Then Shahrazad reached the morning, and fell silent in the telling of her tale…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><small><a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br /><span xmlns:dct=\"http://purl.org/dc/terms/\" property=\"dct:title\">Python Programming for the Humanities</span> by <a xmlns:cc=\"http://creativecommons.org/ns#\" href=\"http://fbkarsdorp.github.io/python-course\" property=\"cc:attributionName\" rel=\"cc:attributionURL\">http://fbkarsdorp.github.io/python-course</a> is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>. Based on a work at <a xmlns:dct=\"http://purl.org/dc/terms/\" href=\"https://github.com/fbkarsdorp/python-course\" rel=\"dct:source\">https://github.com/fbkarsdorp/python-course</a>.</small></p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
