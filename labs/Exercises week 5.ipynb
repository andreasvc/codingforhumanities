{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises week 5\n",
    "\n",
    "In these exercises, you will work with a set of 19th century novels from Project Gutenberg, specifically the 18 texts in the directory `data/gutenberg/training/` (which was part of the chapter 4 notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability in 19th century fiction\n",
    "\n",
    "Read the corpus, split it into sentences, and tokenize and clean it in the same way as in notebook chapter 4.\n",
    "It is useful to put the result into a dictionary `corpus`, with filenames as keys, and the tokenized/sentence-splitted texts as values.\n",
    "\n",
    "Implement a function `readability(text)`. It should use the ARI formula (see the slides from week 1) to estimate the readability of a tokenized text.\n",
    "\n",
    "Apply this function to each novel, and store the results in a dictionary mapping filenames to readability scores.\n",
    "Look at the results:\n",
    "\n",
    "- Who is the most difficult to read?\n",
    "- Do you see interesting or surprising results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment and sensibility\n",
    "\n",
    "Compute a sentiment score for each of the books, using the code on this week's slides. Create a function `sentiment(filename, positive_words, negative_words)` which returns a score for a give filename and sets of sentiment words.\n",
    "\n",
    "- The link on my slides has sentiment lexicons for 81 languages, but not English ... Use the sentiment lexicon made available at: https://github.com/BijoySingh/east/tree/master/east/datasets/opinion_lexicon\n",
    "  Click on the files and press the \"raw\" button to download the file; put them in the `data/` directory.\n",
    "- Note that for this application, we don't care about sentences, so it is easier to read the text as one big list of tokens.\n",
    "- The books have different lengths, is this a problem? If so, can you think of something to correct for this?\n",
    "- Do you see interesting/surprising patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword in context: choose your own adventure\n",
    "\n",
    "Apply some of the Keyword-in-Context methods from NLTK to one or two novels of your choice, see the code on this week's slides and chapter 1 of the NLTK book. Again, for this application, we don't care about sentences, so it is easier to read the text as one big list of tokens. Think of interesting things to explore related to word usage in the novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
