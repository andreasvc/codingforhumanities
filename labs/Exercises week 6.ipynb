{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises week 6\n",
    "\n",
    "In these exercises, you will continue to work with the set of 19th century novels from Project Gutenberg, specifically the 18 texts in the directory `data/gutenberg/training/`.\n",
    "\n",
    "The goals for today are as follows:\n",
    "- Create Pandas `Series` objects from lists and dictionaries.\n",
    "- Rename the labels in the index\n",
    "- Sort the data\n",
    "- Perform arithmetic on Series objects\n",
    "- Make visualizations (bar plots)\n",
    "- Advanced: smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "from glob import glob\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Plotting the readability of 19th century fiction\n",
    "\n",
    "### Creating Series objects\n",
    "\n",
    "Take the readability results from week 5 (a dictionary mapping filenames to readability scores), and put them in a Series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from week 5 here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Series\n",
    "readab = pd.Series(...)\n",
    "# inspect what the Series looks like\n",
    "readab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manipulating the index\n",
    "\n",
    "Your filenames probably include the path which makes them long and cumbersome: `data/gutenberg/training/austen-sense.txt` etc.\n",
    "\n",
    "We can change those labels by changing the `.index` of our Series object. This works as follows. Create a new list with exactly the same number of items as the original index. Finally, replace the index by assigning the new index to it. For example:\n",
    "\n",
    "```python\n",
    "data = pandas.Series([0, 1])\n",
    "newindex = ['a', 'b']\n",
    "data.index = newindex\n",
    "```\n",
    "\n",
    "To make our filenames shorter, we can re-use the function `remove_dir_ext` from Chapter 3 to remove the directories and the extension. Apply this function to all the items in the index. The goal is to have a `Series` object with clear and short names such as `austen-sense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sorting data\n",
    "\n",
    "We can sort our data to easily see which book has the lowest and highest score. Pandas provides two methods for this:\n",
    "\n",
    "```python\n",
    "data.sort_index()\n",
    "data.sort_values()\n",
    "```\n",
    "\n",
    "Try them both. Do you understand the difference? Note that these methods return a *new* sorted copy. If you want to keep the sorted version, you have to assign it:\n",
    "\n",
    "```python\n",
    "data = data.sort_values()\n",
    "```\n",
    "\n",
    "Sort the Series with the readability scores by the scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now provide a simple bar plot. A horizontal bar plot fits best, because this makes the names of the novels more readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plot is missing a label for the x-axis. Here's how to add it:\n",
    "\n",
    "```python\n",
    "ax = data.plot.barh()\n",
    "ax.set_xlabel('My label')\n",
    "```\n",
    "\n",
    "Add the label to your plot. A good label would be \"Readability (ARI)\" which describes the quantity shown and the specific formula that was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Relative sentiment\n",
    "\n",
    "Use the sentiment scores you computed in the exercises of week 5. Put them in a Series, sort them, fix the index, and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will reconsider a question research in the previous week's exercises:\n",
    "\n",
    "- The books have different lengths, is this a problem? If so, can you think of something to correct for this?\n",
    "\n",
    "The answer is yes, we should fix this! We should first know the length of each book. Since the sentiment scores count tokens, the relevant length is the total number of tokens in a text.\n",
    "\n",
    "Count the number of tokens in each file. Create a dictionary with a mapping of filenames to the number of tokens in that file. Put it into a Series object, just as you did for the sentiment scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we now make the sentiment scores for different texts comparable? The answer is that we should calculate the proportion of the sentiment score over the number of tokens:\n",
    "\n",
    "$$\\textrm{sentiment proportion}(\\textrm{text}) = \\frac{\\textrm{sentiment score of text}}{\\textrm{number of words in text}}$$\n",
    "\n",
    "It turns out we can very easily compute this if we have two Series objects, as long as they have the exact same index:\n",
    "\n",
    "```python\n",
    "data1 = pandas.Series(...)\n",
    "data2 = pandas.Series(...)\n",
    "proportion = data1 / data2\n",
    "```\n",
    "\n",
    "Apply this to the sentiment scores and the number of tokens to obtain a 'sentiment proportion' for each novel. Also sort and plot these scores. What differences do you seebetween the normal sentiment scores and the proportional sentiment scores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentiment arcs\n",
    "\n",
    "Let's do something more advanced: we will try to plot the sentiment arc of a text. Instead of summing the sentiment scores into a single number, we will put the sentiment score of each token in a list. This will allow us to track the sentiment over time (at least over what we may call \"text time\").\n",
    "\n",
    "Since this exercises is more advanced, the code is given. Read the code, run it, and try to analyze the results.\n",
    "\n",
    "We pick Sense & Sensibility to analyze. We first load its tokens into a Series object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/gutenberg/training/austen-sense.txt', encoding='utf8') as inp:\n",
    "    austen = nltk.word_tokenize(inp.read().lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a list which contains for each token a number:\n",
    "\n",
    "- -1 if it is a negative word\n",
    "- 1 if it is a positive word\n",
    "- 0 if it is neither\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/positive-words.txt', encoding='utf8') as inp:\n",
    "    positive_words = set(inp.read().splitlines())\n",
    "with open('data/negative-words.txt', encoding='utf8') as inp:\n",
    "    negative_words = set(inp.read().splitlines())\n",
    "\n",
    "def sentiment_arc(filename, positive_words, negative_words):\n",
    "    with open(filename, encoding='utf8') as inp:\n",
    "        tokens = nltk.word_tokenize(inp.read().lower())\n",
    "    sentiment = []\n",
    "    for token in tokens:\n",
    "        if token in positive_words:\n",
    "            sentiment.append(1)\n",
    "        elif token in negative_words:\n",
    "            sentiment.append(-1)\n",
    "        else:\n",
    "            sentiment.append(0)\n",
    "    return sentiment\n",
    "\n",
    "austen_sentim = pd.Series(sentiment_arc(\n",
    "        'data/gutenberg/training/austen-sense.txt',\n",
    "        positive_words, negative_words))\n",
    "austen_sentim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Series provides the raw data for a \"sentiment arc\", but we need to zoom out from individual words to the sentiment over longer stretches of text, say chunks of a 5000 tokens.\n",
    "\n",
    "We can do this by applying some advanced Pandas magic, namely a \"rolling sum\". For each token, we collect the sum of the sentiment scores for the preceding 5000 tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "austen_arc = austen_sentim.rolling(5000).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This gives us a nice plot arc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = austen_arc.plot();\n",
    "ax.set_xlabel('text time')\n",
    "ax.set_ylabel('sentiment score')\n",
    "austen_arc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question now is: is this plot meaningful? Try to look at one or more of the peaks, and see if you can trace it back to a particular part of the novel and perhaps particular events.\n",
    "\n",
    "Note that the numbers on the x-axis are token numbers. If you want to took at the context around token 10000 for example, you could do so as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(' '.join(austen[10000:10100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
